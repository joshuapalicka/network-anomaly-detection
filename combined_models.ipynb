{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import autoencoder.aecExtraFeatures as Z_calculations\n",
    "\n",
    "def addZToPrediction(model, data_point):\n",
    "    encoded = model.encoder(data_point)\n",
    "    reconstruction = model.decoder(encoded)\n",
    "\n",
    "    Z_features = [Z_calculations.getZVector(data_point, reconstruction, encoded)]\n",
    "\n",
    "    Z_features_tensor = tf.convert_to_tensor(Z_features, dtype=tf.float32)\n",
    "    data_point = tf.convert_to_tensor(data_point, dtype=tf.float32)\n",
    "\n",
    "    data_point = tf.concat([data_point, Z_features_tensor], 1)\n",
    "\n",
    "    return data_point"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def isAnomaly(data_point, model_1, model_2, threshold):\n",
    "\n",
    "    # need autoencoder to return boolean isAnomaly\n",
    "    isAnomaly = tf.math.less(tf.keras.losses.mae(model_1(data_point), data_point), threshold)\n",
    "\n",
    "    # if the autoencoder doesn't find anything out of the ordinary, return False\n",
    "    if not isAnomaly:\n",
    "        return False\n",
    "\n",
    "    data_point = addZToPrediction(model_1, data_point)\n",
    "\n",
    "    # if the autoencoder sees something weird, run it through the isolation forest to make sure\n",
    "    return model_2.predict(data_point)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from turtleIsolationForests.preprocessFeatures import preprocess_features\n",
    "\n",
    "train_dataframe = pd.read_csv(\"eda_simple_classification/network_data_mod_train.csv\", index_col=0)\n",
    "test_dataframe = pd.read_csv(\"eda_simple_classification/network_data_mod_test.csv\", index_col=0)\n",
    "\n",
    "train_data, test_data, train_labels, test_labels = preprocess_features(train_dataframe, test_dataframe)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#train_data, test_data, train_labels, test_labels = train_data[:1000], test_data[:1000], train_labels[:1000], test_labels[:1000]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "np_train_labels = train_labels.to_numpy()\n",
    "np_test_labels = test_labels.to_numpy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "np_train_data = train_data.to_numpy()\n",
    "np_test_data = test_data.to_numpy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "np_train_data = tf.cast(np_train_data, tf.float32)\n",
    "np_test_data = tf.cast(np_test_data, tf.float32)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "np_train_labels = np_train_labels.astype(bool)\n",
    "np_test_labels = np_test_labels.astype(bool)\n",
    "\n",
    "normal_train_data = np_train_data[np_train_labels]\n",
    "normal_test_data = np_test_data[np_test_labels]\n",
    "\n",
    "anomalous_train_data = np_train_data[~np_train_labels]\n",
    "anomalous_test_data = np_test_data[~np_test_labels]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from autoencoder.autoencoder import AnomalyDetector\n",
    "autoencoder = AnomalyDetector()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adam', loss='mae')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Tensor(\"anomaly_detector/sequential/dense_2/Relu:0\", shape=(None, 8), dtype=float32)\n",
      "Tensor(\"anomaly_detector/sequential_1/dense_5/Sigmoid:0\", shape=(None, 46), dtype=float32)\n",
      "Tensor(\"anomaly_detector/sequential/dense_2/Relu:0\", shape=(None, 8), dtype=float32)\n",
      "Tensor(\"anomaly_detector/sequential_1/dense_5/Sigmoid:0\", shape=(None, 46), dtype=float32)\n",
      "2081/2105 [============================>.] - ETA: 0s - loss: 0.3031Tensor(\"anomaly_detector/sequential/dense_2/Relu:0\", shape=(None, 8), dtype=float32)\n",
      "Tensor(\"anomaly_detector/sequential_1/dense_5/Sigmoid:0\", shape=(None, 46), dtype=float32)\n",
      "2105/2105 [==============================] - 5s 2ms/step - loss: 0.3030 - val_loss: 0.4182\n",
      "Epoch 2/100\n",
      "2105/2105 [==============================] - 4s 2ms/step - loss: 0.2856 - val_loss: 0.4077\n",
      "Epoch 3/100\n",
      "2105/2105 [==============================] - 4s 2ms/step - loss: 0.2848 - val_loss: 0.4058\n",
      "Epoch 4/100\n",
      "2105/2105 [==============================] - 4s 2ms/step - loss: 0.2788 - val_loss: 0.4008\n",
      "Epoch 5/100\n",
      "2105/2105 [==============================] - 4s 2ms/step - loss: 0.2756 - val_loss: 0.4005\n",
      "Epoch 6/100\n",
      "2105/2105 [==============================] - 3s 2ms/step - loss: 0.2755 - val_loss: 0.4005\n",
      "Epoch 7/100\n",
      "2105/2105 [==============================] - 4s 2ms/step - loss: 0.2754 - val_loss: 0.3994\n",
      "Epoch 8/100\n",
      "2105/2105 [==============================] - 4s 2ms/step - loss: 0.2753 - val_loss: 0.4006\n",
      "Epoch 9/100\n",
      "2105/2105 [==============================] - 4s 2ms/step - loss: 0.2753 - val_loss: 0.4006\n",
      "Epoch 10/100\n",
      "2105/2105 [==============================] - 4s 2ms/step - loss: 0.2752 - val_loss: 0.3986\n",
      "Epoch 11/100\n",
      "2105/2105 [==============================] - 4s 2ms/step - loss: 0.2732 - val_loss: 0.3986\n",
      "Epoch 12/100\n",
      "2105/2105 [==============================] - 4s 2ms/step - loss: 0.2728 - val_loss: 0.3980\n",
      "Epoch 13/100\n",
      "2105/2105 [==============================] - 4s 2ms/step - loss: 0.2727 - val_loss: 0.3976\n",
      "Epoch 14/100\n",
      "2105/2105 [==============================] - 5s 2ms/step - loss: 0.2721 - val_loss: 0.3975\n",
      "Epoch 15/100\n",
      "2105/2105 [==============================] - 5s 2ms/step - loss: 0.2716 - val_loss: 0.3984\n",
      "Epoch 16/100\n",
      "2105/2105 [==============================] - 5s 2ms/step - loss: 0.2715 - val_loss: 0.3985\n",
      "Epoch 17/100\n",
      "2105/2105 [==============================] - 5s 2ms/step - loss: 0.2715 - val_loss: 0.4005\n",
      "Epoch 18/100\n",
      "2105/2105 [==============================] - 5s 2ms/step - loss: 0.2714 - val_loss: 0.4004\n",
      "Epoch 19/100\n",
      "2105/2105 [==============================] - 5s 2ms/step - loss: 0.2714 - val_loss: 0.4004\n",
      "Epoch 20/100\n",
      "2105/2105 [==============================] - 4s 2ms/step - loss: 0.2714 - val_loss: 0.3997\n",
      "Epoch 21/100\n",
      "2105/2105 [==============================] - 4s 2ms/step - loss: 0.2714 - val_loss: 0.4001\n",
      "Epoch 22/100\n",
      "2105/2105 [==============================] - 4s 2ms/step - loss: 0.2713 - val_loss: 0.4000\n",
      "Epoch 23/100\n",
      "2105/2105 [==============================] - 4s 2ms/step - loss: 0.2713 - val_loss: 0.4007\n",
      "Epoch 24/100\n",
      "2105/2105 [==============================] - 4s 2ms/step - loss: 0.2712 - val_loss: 0.4002\n",
      "Epoch 25/100\n",
      "2105/2105 [==============================] - 4s 2ms/step - loss: 0.2712 - val_loss: 0.3999\n",
      "Epoch 26/100\n",
      "2105/2105 [==============================] - 4s 2ms/step - loss: 0.2712 - val_loss: 0.3984\n",
      "Epoch 27/100\n",
      "2105/2105 [==============================] - 4s 2ms/step - loss: 0.2712 - val_loss: 0.3996\n",
      "Epoch 28/100\n",
      "2105/2105 [==============================] - 4s 2ms/step - loss: 0.2711 - val_loss: 0.3977\n",
      "Epoch 29/100\n",
      "2105/2105 [==============================] - 4s 2ms/step - loss: 0.2711 - val_loss: 0.3985\n",
      "Epoch 30/100\n",
      "2105/2105 [==============================] - 4s 2ms/step - loss: 0.2711 - val_loss: 0.3990\n",
      "Epoch 31/100\n",
      "2105/2105 [==============================] - 4s 2ms/step - loss: 0.2711 - val_loss: 0.3966\n",
      "Epoch 32/100\n",
      "2105/2105 [==============================] - 4s 2ms/step - loss: 0.2710 - val_loss: 0.3959\n",
      "Epoch 33/100\n",
      "2105/2105 [==============================] - 4s 2ms/step - loss: 0.2710 - val_loss: 0.3976\n",
      "Epoch 34/100\n",
      "2105/2105 [==============================] - 4s 2ms/step - loss: 0.2710 - val_loss: 0.3978\n",
      "Epoch 35/100\n",
      "2105/2105 [==============================] - 4s 2ms/step - loss: 0.2710 - val_loss: 0.3965\n",
      "Epoch 36/100\n",
      "2105/2105 [==============================] - 4s 2ms/step - loss: 0.2710 - val_loss: 0.3981\n",
      "Epoch 37/100\n",
      "2105/2105 [==============================] - 4s 2ms/step - loss: 0.2710 - val_loss: 0.3945\n",
      "Epoch 38/100\n",
      "2105/2105 [==============================] - 4s 2ms/step - loss: 0.2710 - val_loss: 0.3952\n",
      "Epoch 39/100\n",
      "2105/2105 [==============================] - 4s 2ms/step - loss: 0.2710 - val_loss: 0.3974\n",
      "Epoch 40/100\n",
      "2105/2105 [==============================] - 4s 2ms/step - loss: 0.2710 - val_loss: 0.3974\n",
      "Epoch 41/100\n",
      "2105/2105 [==============================] - 3s 2ms/step - loss: 0.2709 - val_loss: 0.3973\n",
      "Epoch 42/100\n",
      "2105/2105 [==============================] - 3s 2ms/step - loss: 0.2709 - val_loss: 0.3970\n",
      "Epoch 43/100\n",
      "2105/2105 [==============================] - 3s 2ms/step - loss: 0.2709 - val_loss: 0.3971\n",
      "Epoch 44/100\n",
      "2105/2105 [==============================] - 3s 2ms/step - loss: 0.2709 - val_loss: 0.3972\n",
      "Epoch 45/100\n",
      "2105/2105 [==============================] - 3s 2ms/step - loss: 0.2708 - val_loss: 0.3972\n",
      "Epoch 46/100\n",
      "2105/2105 [==============================] - 3s 1ms/step - loss: 0.2708 - val_loss: 0.3930\n",
      "Epoch 47/100\n",
      "2105/2105 [==============================] - 3s 1ms/step - loss: 0.2708 - val_loss: 0.3939\n",
      "Epoch 48/100\n",
      "2105/2105 [==============================] - 3s 1ms/step - loss: 0.2708 - val_loss: 0.3922\n",
      "Epoch 49/100\n",
      "2105/2105 [==============================] - 3s 1ms/step - loss: 0.2707 - val_loss: 0.3911\n",
      "Epoch 50/100\n",
      "2105/2105 [==============================] - 3s 1ms/step - loss: 0.2707 - val_loss: 0.3931\n",
      "Epoch 51/100\n",
      "2105/2105 [==============================] - 3s 1ms/step - loss: 0.2707 - val_loss: 0.3949\n",
      "Epoch 52/100\n",
      "2105/2105 [==============================] - 3s 2ms/step - loss: 0.2707 - val_loss: 0.3932\n",
      "Epoch 53/100\n",
      "2105/2105 [==============================] - 3s 2ms/step - loss: 0.2707 - val_loss: 0.3937\n",
      "Epoch 54/100\n",
      "2105/2105 [==============================] - 3s 2ms/step - loss: 0.2707 - val_loss: 0.3930\n",
      "Epoch 55/100\n",
      "2105/2105 [==============================] - 3s 2ms/step - loss: 0.2707 - val_loss: 0.3924\n",
      "Epoch 56/100\n",
      "2105/2105 [==============================] - 3s 2ms/step - loss: 0.2707 - val_loss: 0.3922\n",
      "Epoch 57/100\n",
      "2105/2105 [==============================] - 3s 2ms/step - loss: 0.2707 - val_loss: 0.3930\n",
      "Epoch 58/100\n",
      "2105/2105 [==============================] - 3s 2ms/step - loss: 0.2707 - val_loss: 0.3930\n",
      "Epoch 59/100\n",
      "2105/2105 [==============================] - 3s 1ms/step - loss: 0.2706 - val_loss: 0.3930\n",
      "Epoch 60/100\n",
      "2105/2105 [==============================] - 3s 1ms/step - loss: 0.2706 - val_loss: 0.3934\n",
      "Epoch 61/100\n",
      "2105/2105 [==============================] - 3s 1ms/step - loss: 0.2706 - val_loss: 0.3948\n",
      "Epoch 62/100\n",
      "2105/2105 [==============================] - 3s 1ms/step - loss: 0.2706 - val_loss: 0.3930\n",
      "Epoch 63/100\n",
      "2105/2105 [==============================] - 3s 1ms/step - loss: 0.2706 - val_loss: 0.3951\n",
      "Epoch 64/100\n",
      "2105/2105 [==============================] - 3s 2ms/step - loss: 0.2706 - val_loss: 0.3932\n",
      "Epoch 65/100\n",
      "2105/2105 [==============================] - 3s 1ms/step - loss: 0.2706 - val_loss: 0.3914\n",
      "Epoch 66/100\n",
      "2105/2105 [==============================] - 3s 1ms/step - loss: 0.2706 - val_loss: 0.3948\n",
      "Epoch 67/100\n",
      "2105/2105 [==============================] - 3s 2ms/step - loss: 0.2700 - val_loss: 0.3899\n",
      "Epoch 68/100\n",
      "2105/2105 [==============================] - 3s 1ms/step - loss: 0.2693 - val_loss: 0.3911\n",
      "Epoch 69/100\n",
      "2105/2105 [==============================] - 3s 2ms/step - loss: 0.2664 - val_loss: 0.3900\n",
      "Epoch 70/100\n",
      "2105/2105 [==============================] - 3s 2ms/step - loss: 0.2661 - val_loss: 0.3894\n",
      "Epoch 71/100\n",
      "2105/2105 [==============================] - 3s 2ms/step - loss: 0.2661 - val_loss: 0.3894\n",
      "Epoch 72/100\n",
      "2105/2105 [==============================] - 3s 2ms/step - loss: 0.2661 - val_loss: 0.3879\n",
      "Epoch 73/100\n",
      "2105/2105 [==============================] - 3s 2ms/step - loss: 0.2661 - val_loss: 0.3893\n",
      "Epoch 74/100\n",
      "2105/2105 [==============================] - 3s 2ms/step - loss: 0.2660 - val_loss: 0.3876\n",
      "Epoch 75/100\n",
      "2105/2105 [==============================] - 3s 2ms/step - loss: 0.2660 - val_loss: 0.3879\n",
      "Epoch 76/100\n",
      "2105/2105 [==============================] - 3s 2ms/step - loss: 0.2660 - val_loss: 0.3870\n",
      "Epoch 77/100\n",
      "2105/2105 [==============================] - 3s 2ms/step - loss: 0.2660 - val_loss: 0.3876\n",
      "Epoch 78/100\n",
      "2105/2105 [==============================] - 3s 2ms/step - loss: 0.2660 - val_loss: 0.3883\n",
      "Epoch 79/100\n",
      "2105/2105 [==============================] - 3s 2ms/step - loss: 0.2660 - val_loss: 0.3873\n",
      "Epoch 80/100\n",
      "2105/2105 [==============================] - 3s 2ms/step - loss: 0.2660 - val_loss: 0.3868\n",
      "Epoch 81/100\n",
      "2105/2105 [==============================] - 4s 2ms/step - loss: 0.2660 - val_loss: 0.3874\n",
      "Epoch 82/100\n",
      "2105/2105 [==============================] - 3s 2ms/step - loss: 0.2660 - val_loss: 0.3875\n",
      "Epoch 83/100\n",
      "2105/2105 [==============================] - 4s 2ms/step - loss: 0.2660 - val_loss: 0.3876\n",
      "Epoch 84/100\n",
      "2105/2105 [==============================] - 3s 2ms/step - loss: 0.2659 - val_loss: 0.3878\n",
      "Epoch 85/100\n",
      "2105/2105 [==============================] - 4s 2ms/step - loss: 0.2660 - val_loss: 0.3875\n",
      "Epoch 86/100\n",
      "2105/2105 [==============================] - 3s 2ms/step - loss: 0.2660 - val_loss: 0.3865\n",
      "Epoch 87/100\n",
      "2105/2105 [==============================] - 3s 2ms/step - loss: 0.2659 - val_loss: 0.3864\n",
      "Epoch 88/100\n",
      "2105/2105 [==============================] - 4s 2ms/step - loss: 0.2659 - val_loss: 0.3862\n",
      "Epoch 89/100\n",
      "2105/2105 [==============================] - 4s 2ms/step - loss: 0.2659 - val_loss: 0.3861\n",
      "Epoch 90/100\n",
      "2105/2105 [==============================] - 3s 2ms/step - loss: 0.2659 - val_loss: 0.3863\n",
      "Epoch 91/100\n",
      "2105/2105 [==============================] - 3s 2ms/step - loss: 0.2659 - val_loss: 0.3874\n",
      "Epoch 92/100\n",
      "2105/2105 [==============================] - 3s 2ms/step - loss: 0.2659 - val_loss: 0.3878\n",
      "Epoch 93/100\n",
      "2105/2105 [==============================] - 3s 2ms/step - loss: 0.2659 - val_loss: 0.3861\n",
      "Epoch 94/100\n",
      "2105/2105 [==============================] - 4s 2ms/step - loss: 0.2659 - val_loss: 0.3863\n",
      "Epoch 95/100\n",
      "2105/2105 [==============================] - 3s 2ms/step - loss: 0.2659 - val_loss: 0.3877\n",
      "Epoch 96/100\n",
      "2105/2105 [==============================] - 3s 2ms/step - loss: 0.2659 - val_loss: 0.3843\n",
      "Epoch 97/100\n",
      "2105/2105 [==============================] - 4s 2ms/step - loss: 0.2659 - val_loss: 0.3869\n",
      "Epoch 98/100\n",
      "2105/2105 [==============================] - 4s 2ms/step - loss: 0.2659 - val_loss: 0.3862\n",
      "Epoch 99/100\n",
      "2105/2105 [==============================] - 3s 2ms/step - loss: 0.2659 - val_loss: 0.3860\n",
      "Epoch 100/100\n",
      "2105/2105 [==============================] - 4s 2ms/step - loss: 0.2659 - val_loss: 0.3864\n"
     ]
    }
   ],
   "source": [
    "history = autoencoder.fit(normal_train_data, normal_train_data,\n",
    "          epochs=100,\n",
    "          validation_data=(test_data, test_data),\n",
    "          shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.title(\"Training Loss & Validation Loss over epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "#plt.ylim([0,.0008])\n",
    "plt.legend([\"Loss\", \"Validation Loss\"])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"anomaly_detector/sequential/dense_2/Relu:0\", shape=(None, 8), dtype=float32)\n",
      "Tensor(\"anomaly_detector/sequential_1/dense_5/Sigmoid:0\", shape=(None, 46), dtype=float32)\n",
      "2105/2105 [==============================] - 2s 989us/step\n",
      "Threshold:  0.43404865\n"
     ]
    }
   ],
   "source": [
    "reconstructions = autoencoder.predict(normal_train_data)\n",
    "train_loss = tf.keras.losses.mae(reconstructions, normal_train_data)\n",
    "threshold = np.mean(train_loss) + np.std(train_loss)\n",
    "print(\"Threshold: \", threshold)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def predict(model, data, threshold):\n",
    "  reconstructions = model(data)\n",
    "  loss = tf.keras.losses.mae(data, reconstructions) # 0 = anomaly (same as data)\n",
    "  return tf.math.less(loss, threshold)\n",
    "\n",
    "def print_stats(predictions, labels):\n",
    "  print(\"Accuracy = {}\".format(accuracy_score(labels, predictions)))\n",
    "  print(\"Precision = {}\".format(precision_score(labels, predictions)))\n",
    "  print(\"Recall = {}\".format(recall_score(labels, predictions)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.5 ms ± 1.27 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "%%capture\n",
    "\n",
    "predict(autoencoder, np_test_data, threshold)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "test_predictions = predict(autoencoder, np_test_data, threshold)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Stats for whole dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.7363378282469837\n",
      "Precision = 0.6248591315876699\n",
      "Recall = 0.9706518381217176\n"
     ]
    }
   ],
   "source": [
    "print_stats(test_predictions, test_labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7174 5659]\n",
      " [ 285 9426]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(test_labels, test_predictions))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "test_labels_swap = []\n",
    "for i in range(len(test_labels)):\n",
    "    if test_labels[i]:\n",
    "        test_labels_swap.append(False)\n",
    "    else:\n",
    "        test_labels_swap.append(True)\n",
    "\n",
    "test_labels = test_labels_swap\n",
    "np_test_labels = test_labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Stats for all predicted anomalous data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true anomalies: 9426\n",
      "false anomalies: 5659\n",
      "false normals: 285\n",
      "true normals: 7174\n",
      "precision: 0.6248591315876699\n",
      "recall: 0.9706518381217176\n",
      "f1-score: 0.7602839167607679\n"
     ]
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "from turtleIsolationForests.printResults import print_results\n",
    "\n",
    "test_predictions_np = test_predictions.numpy()\n",
    "\n",
    "autoec_predictions = DataFrame()\n",
    "\n",
    "autoec_predictions['predicted_as_anomaly'] = test_predictions_np\n",
    "autoec_predictions['is_normal'] = test_labels\n",
    "\n",
    "print_results(autoec_predictions)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def addZToData(data, model):\n",
    "    data_with_Z = []\n",
    "    for i in range(1, len(data)+1):\n",
    "        data_with_Z.append(addZToPrediction(model, data[i-1:i]))\n",
    "\n",
    "    data_with_Z_rf = []\n",
    "    for i in range(len(data_with_Z)):\n",
    "        data_with_Z_rf.append(np.append(np_train_data[:][:][i].numpy().reshape(1,46).squeeze(), data_with_Z[i]))\n",
    "\n",
    "    return pd.DataFrame(data_with_Z_rf)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "train_data_with_Z_df = addZToData(np_train_data, autoencoder)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "              0         1        2         3         4         5         6   \\\n0      -0.019113 -0.312889 -0.11205 -0.028606 -0.139982 -0.618438 -0.053906   \n1      -0.019113 -0.312889 -0.11205 -0.028606 -0.139982 -0.618438 -0.053906   \n2      -0.019113 -0.312889 -0.11205 -0.028606 -0.139982  1.616978 -0.053906   \n3      -0.019113 -0.312889 -0.11205 -0.028606 -0.139982 -0.618438 -0.053906   \n4      -0.019113 -0.312889 -0.11205 -0.028606 -0.139982 -0.618438 -0.053906   \n...          ...       ...      ...       ...       ...       ...       ...   \n125968 -0.019113 -0.312889 -0.11205 -0.028606 -0.139982  1.616978 -0.053906   \n125969 -0.019113 -0.312889 -0.11205 -0.028606 -0.139982 -0.618438 -0.053906   \n125970 -0.019113 -0.312889 -0.11205 -0.028606 -0.139982 -0.618438 -0.053906   \n125971 -0.019113 -0.312889 -0.11205 -0.028606 -0.139982  1.616978 -0.053906   \n125972 -0.019113 -0.312889 -0.11205 -0.028606 -0.139982 -0.618438 -0.053906   \n\n              7         8         9   ...        85        86        87  \\\n0      -0.031767 -0.019726  0.825150  ... -0.280282  0.069972 -0.289103   \n1      -0.031767 -0.019726  0.825150  ...  2.736852  2.367737 -0.289103   \n2      -0.031767 -0.019726 -1.211901  ... -0.174417 -0.480197 -0.289103   \n3      -0.031767 -0.019726  0.825150  ... -0.439078 -0.383108  0.066252   \n4      -0.031767 -0.019726  0.825150  ... -0.439078 -0.480197 -0.289103   \n...          ...       ...       ...  ...       ...       ...       ...   \n125968 -0.031767 -0.019726 -1.211901  ... -0.121485 -0.480197 -0.289103   \n125969 -0.031767 -0.019726  0.825150  ... -0.386146 -0.447834 -0.289103   \n125970 -0.031767 -0.019726  0.825150  ... -0.121485 -0.480197 -0.289103   \n125971 -0.031767 -0.019726 -1.211901  ... -0.174417 -0.480197 -0.289103   \n125972 -0.031767 -0.019726  0.825150  ... -0.280282  0.490690 -0.289103   \n\n              88        89        90        91        92        93        94  \n0      -0.639532 -0.624871 -0.224532 -0.376387  6.788234  2.396257  0.426340  \n1      -0.639532 -0.624871 -0.387635 -0.376387  5.030957  4.649899  0.240919  \n2       1.608759  1.618955 -0.387635 -0.376387  8.147187  4.531855  0.002381  \n3      -0.572083 -0.602433 -0.387635 -0.345084  2.860933  2.350209  0.707758  \n4      -0.639532 -0.624871 -0.387635 -0.376387  0.804795  1.926370  0.788824  \n...          ...       ...       ...       ...       ...       ...       ...  \n125968  1.608759  1.618955 -0.387635 -0.376387  7.380088  4.536170 -0.030376  \n125969 -0.639532 -0.624871 -0.387635 -0.376387  0.944327  2.178603  0.680804  \n125970  0.979238 -0.624871 -0.355014 -0.376387  6.999238  2.441846  0.597644  \n125971  1.608759  1.618955 -0.387635 -0.376387  8.420595  4.608039 -0.025003  \n125972 -0.639532 -0.624871 -0.387635 -0.376387  5.286558  2.116265  0.652306  \n\n[125973 rows x 95 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>85</th>\n      <th>86</th>\n      <th>87</th>\n      <th>88</th>\n      <th>89</th>\n      <th>90</th>\n      <th>91</th>\n      <th>92</th>\n      <th>93</th>\n      <th>94</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>-0.618438</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>0.825150</td>\n      <td>...</td>\n      <td>-0.280282</td>\n      <td>0.069972</td>\n      <td>-0.289103</td>\n      <td>-0.639532</td>\n      <td>-0.624871</td>\n      <td>-0.224532</td>\n      <td>-0.376387</td>\n      <td>6.788234</td>\n      <td>2.396257</td>\n      <td>0.426340</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>-0.618438</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>0.825150</td>\n      <td>...</td>\n      <td>2.736852</td>\n      <td>2.367737</td>\n      <td>-0.289103</td>\n      <td>-0.639532</td>\n      <td>-0.624871</td>\n      <td>-0.387635</td>\n      <td>-0.376387</td>\n      <td>5.030957</td>\n      <td>4.649899</td>\n      <td>0.240919</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>1.616978</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>-1.211901</td>\n      <td>...</td>\n      <td>-0.174417</td>\n      <td>-0.480197</td>\n      <td>-0.289103</td>\n      <td>1.608759</td>\n      <td>1.618955</td>\n      <td>-0.387635</td>\n      <td>-0.376387</td>\n      <td>8.147187</td>\n      <td>4.531855</td>\n      <td>0.002381</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>-0.618438</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>0.825150</td>\n      <td>...</td>\n      <td>-0.439078</td>\n      <td>-0.383108</td>\n      <td>0.066252</td>\n      <td>-0.572083</td>\n      <td>-0.602433</td>\n      <td>-0.387635</td>\n      <td>-0.345084</td>\n      <td>2.860933</td>\n      <td>2.350209</td>\n      <td>0.707758</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>-0.618438</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>0.825150</td>\n      <td>...</td>\n      <td>-0.439078</td>\n      <td>-0.480197</td>\n      <td>-0.289103</td>\n      <td>-0.639532</td>\n      <td>-0.624871</td>\n      <td>-0.387635</td>\n      <td>-0.376387</td>\n      <td>0.804795</td>\n      <td>1.926370</td>\n      <td>0.788824</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>125968</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>1.616978</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>-1.211901</td>\n      <td>...</td>\n      <td>-0.121485</td>\n      <td>-0.480197</td>\n      <td>-0.289103</td>\n      <td>1.608759</td>\n      <td>1.618955</td>\n      <td>-0.387635</td>\n      <td>-0.376387</td>\n      <td>7.380088</td>\n      <td>4.536170</td>\n      <td>-0.030376</td>\n    </tr>\n    <tr>\n      <th>125969</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>-0.618438</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>0.825150</td>\n      <td>...</td>\n      <td>-0.386146</td>\n      <td>-0.447834</td>\n      <td>-0.289103</td>\n      <td>-0.639532</td>\n      <td>-0.624871</td>\n      <td>-0.387635</td>\n      <td>-0.376387</td>\n      <td>0.944327</td>\n      <td>2.178603</td>\n      <td>0.680804</td>\n    </tr>\n    <tr>\n      <th>125970</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>-0.618438</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>0.825150</td>\n      <td>...</td>\n      <td>-0.121485</td>\n      <td>-0.480197</td>\n      <td>-0.289103</td>\n      <td>0.979238</td>\n      <td>-0.624871</td>\n      <td>-0.355014</td>\n      <td>-0.376387</td>\n      <td>6.999238</td>\n      <td>2.441846</td>\n      <td>0.597644</td>\n    </tr>\n    <tr>\n      <th>125971</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>1.616978</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>-1.211901</td>\n      <td>...</td>\n      <td>-0.174417</td>\n      <td>-0.480197</td>\n      <td>-0.289103</td>\n      <td>1.608759</td>\n      <td>1.618955</td>\n      <td>-0.387635</td>\n      <td>-0.376387</td>\n      <td>8.420595</td>\n      <td>4.608039</td>\n      <td>-0.025003</td>\n    </tr>\n    <tr>\n      <th>125972</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>-0.618438</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>0.825150</td>\n      <td>...</td>\n      <td>-0.280282</td>\n      <td>0.490690</td>\n      <td>-0.289103</td>\n      <td>-0.639532</td>\n      <td>-0.624871</td>\n      <td>-0.387635</td>\n      <td>-0.376387</td>\n      <td>5.286558</td>\n      <td>2.116265</td>\n      <td>0.652306</td>\n    </tr>\n  </tbody>\n</table>\n<p>125973 rows × 95 columns</p>\n</div>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_with_Z_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "np_test_labels = np.array(np_test_labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "predicted_anomalous = np_test_data[test_predictions.numpy()]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "              0         1        2         3         4         5         6   \\\n0      -0.019113 -0.312889 -0.11205 -0.028606 -0.139982 -0.618438 -0.053906   \n1      -0.019113 -0.312889 -0.11205 -0.028606 -0.139982 -0.618438 -0.053906   \n2      -0.019113 -0.312889 -0.11205 -0.028606 -0.139982  1.616978 -0.053906   \n3      -0.019113 -0.312889 -0.11205 -0.028606 -0.139982 -0.618438 -0.053906   \n4      -0.019113 -0.312889 -0.11205 -0.028606 -0.139982 -0.618438 -0.053906   \n...          ...       ...      ...       ...       ...       ...       ...   \n125968 -0.019113 -0.312889 -0.11205 -0.028606 -0.139982  1.616978 -0.053906   \n125969 -0.019113 -0.312889 -0.11205 -0.028606 -0.139982 -0.618438 -0.053906   \n125970 -0.019113 -0.312889 -0.11205 -0.028606 -0.139982 -0.618438 -0.053906   \n125971 -0.019113 -0.312889 -0.11205 -0.028606 -0.139982  1.616978 -0.053906   \n125972 -0.019113 -0.312889 -0.11205 -0.028606 -0.139982 -0.618438 -0.053906   \n\n              7         8         9   ...        85        86        87  \\\n0      -0.031767 -0.019726  0.825150  ... -0.280282  0.069972 -0.289103   \n1      -0.031767 -0.019726  0.825150  ...  2.736852  2.367737 -0.289103   \n2      -0.031767 -0.019726 -1.211901  ... -0.174417 -0.480197 -0.289103   \n3      -0.031767 -0.019726  0.825150  ... -0.439078 -0.383108  0.066252   \n4      -0.031767 -0.019726  0.825150  ... -0.439078 -0.480197 -0.289103   \n...          ...       ...       ...  ...       ...       ...       ...   \n125968 -0.031767 -0.019726 -1.211901  ... -0.121485 -0.480197 -0.289103   \n125969 -0.031767 -0.019726  0.825150  ... -0.386146 -0.447834 -0.289103   \n125970 -0.031767 -0.019726  0.825150  ... -0.121485 -0.480197 -0.289103   \n125971 -0.031767 -0.019726 -1.211901  ... -0.174417 -0.480197 -0.289103   \n125972 -0.031767 -0.019726  0.825150  ... -0.280282  0.490690 -0.289103   \n\n              88        89        90        91        92        93        94  \n0      -0.639532 -0.624871 -0.224532 -0.376387  6.788234  2.396257  0.426340  \n1      -0.639532 -0.624871 -0.387635 -0.376387  5.030957  4.649899  0.240919  \n2       1.608759  1.618955 -0.387635 -0.376387  8.147187  4.531855  0.002381  \n3      -0.572083 -0.602433 -0.387635 -0.345084  2.860933  2.350209  0.707758  \n4      -0.639532 -0.624871 -0.387635 -0.376387  0.804795  1.926370  0.788824  \n...          ...       ...       ...       ...       ...       ...       ...  \n125968  1.608759  1.618955 -0.387635 -0.376387  7.380088  4.536170 -0.030376  \n125969 -0.639532 -0.624871 -0.387635 -0.376387  0.944327  2.178603  0.680804  \n125970  0.979238 -0.624871 -0.355014 -0.376387  6.999238  2.441846  0.597644  \n125971  1.608759  1.618955 -0.387635 -0.376387  8.420595  4.608039 -0.025003  \n125972 -0.639532 -0.624871 -0.387635 -0.376387  5.286558  2.116265  0.652306  \n\n[125973 rows x 95 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>85</th>\n      <th>86</th>\n      <th>87</th>\n      <th>88</th>\n      <th>89</th>\n      <th>90</th>\n      <th>91</th>\n      <th>92</th>\n      <th>93</th>\n      <th>94</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>-0.618438</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>0.825150</td>\n      <td>...</td>\n      <td>-0.280282</td>\n      <td>0.069972</td>\n      <td>-0.289103</td>\n      <td>-0.639532</td>\n      <td>-0.624871</td>\n      <td>-0.224532</td>\n      <td>-0.376387</td>\n      <td>6.788234</td>\n      <td>2.396257</td>\n      <td>0.426340</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>-0.618438</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>0.825150</td>\n      <td>...</td>\n      <td>2.736852</td>\n      <td>2.367737</td>\n      <td>-0.289103</td>\n      <td>-0.639532</td>\n      <td>-0.624871</td>\n      <td>-0.387635</td>\n      <td>-0.376387</td>\n      <td>5.030957</td>\n      <td>4.649899</td>\n      <td>0.240919</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>1.616978</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>-1.211901</td>\n      <td>...</td>\n      <td>-0.174417</td>\n      <td>-0.480197</td>\n      <td>-0.289103</td>\n      <td>1.608759</td>\n      <td>1.618955</td>\n      <td>-0.387635</td>\n      <td>-0.376387</td>\n      <td>8.147187</td>\n      <td>4.531855</td>\n      <td>0.002381</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>-0.618438</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>0.825150</td>\n      <td>...</td>\n      <td>-0.439078</td>\n      <td>-0.383108</td>\n      <td>0.066252</td>\n      <td>-0.572083</td>\n      <td>-0.602433</td>\n      <td>-0.387635</td>\n      <td>-0.345084</td>\n      <td>2.860933</td>\n      <td>2.350209</td>\n      <td>0.707758</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>-0.618438</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>0.825150</td>\n      <td>...</td>\n      <td>-0.439078</td>\n      <td>-0.480197</td>\n      <td>-0.289103</td>\n      <td>-0.639532</td>\n      <td>-0.624871</td>\n      <td>-0.387635</td>\n      <td>-0.376387</td>\n      <td>0.804795</td>\n      <td>1.926370</td>\n      <td>0.788824</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>125968</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>1.616978</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>-1.211901</td>\n      <td>...</td>\n      <td>-0.121485</td>\n      <td>-0.480197</td>\n      <td>-0.289103</td>\n      <td>1.608759</td>\n      <td>1.618955</td>\n      <td>-0.387635</td>\n      <td>-0.376387</td>\n      <td>7.380088</td>\n      <td>4.536170</td>\n      <td>-0.030376</td>\n    </tr>\n    <tr>\n      <th>125969</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>-0.618438</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>0.825150</td>\n      <td>...</td>\n      <td>-0.386146</td>\n      <td>-0.447834</td>\n      <td>-0.289103</td>\n      <td>-0.639532</td>\n      <td>-0.624871</td>\n      <td>-0.387635</td>\n      <td>-0.376387</td>\n      <td>0.944327</td>\n      <td>2.178603</td>\n      <td>0.680804</td>\n    </tr>\n    <tr>\n      <th>125970</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>-0.618438</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>0.825150</td>\n      <td>...</td>\n      <td>-0.121485</td>\n      <td>-0.480197</td>\n      <td>-0.289103</td>\n      <td>0.979238</td>\n      <td>-0.624871</td>\n      <td>-0.355014</td>\n      <td>-0.376387</td>\n      <td>6.999238</td>\n      <td>2.441846</td>\n      <td>0.597644</td>\n    </tr>\n    <tr>\n      <th>125971</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>1.616978</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>-1.211901</td>\n      <td>...</td>\n      <td>-0.174417</td>\n      <td>-0.480197</td>\n      <td>-0.289103</td>\n      <td>1.608759</td>\n      <td>1.618955</td>\n      <td>-0.387635</td>\n      <td>-0.376387</td>\n      <td>8.420595</td>\n      <td>4.608039</td>\n      <td>-0.025003</td>\n    </tr>\n    <tr>\n      <th>125972</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>-0.618438</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>0.825150</td>\n      <td>...</td>\n      <td>-0.280282</td>\n      <td>0.490690</td>\n      <td>-0.289103</td>\n      <td>-0.639532</td>\n      <td>-0.624871</td>\n      <td>-0.387635</td>\n      <td>-0.376387</td>\n      <td>5.286558</td>\n      <td>2.116265</td>\n      <td>0.652306</td>\n    </tr>\n  </tbody>\n</table>\n<p>125973 rows × 95 columns</p>\n</div>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_with_Z_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "predicted_anomalous_labels = np_test_labels[test_predictions.numpy()]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "anomalous_test_data_with_Z_df = addZToData(predicted_anomalous, autoencoder)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "contamination = sum(train_labels == 0) / len(train_labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "# need test_predictions replaced at indices that autoencoder predicted anomalies"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "             0         1        2         3         4         5         6   \\\n0     -0.019113 -0.312889 -0.11205 -0.028606 -0.139982 -0.618438 -0.053906   \n1     -0.019113 -0.312889 -0.11205 -0.028606 -0.139982 -0.618438 -0.053906   \n2     -0.019113 -0.312889 -0.11205 -0.028606 -0.139982  1.616978 -0.053906   \n3     -0.019113 -0.312889 -0.11205 -0.028606 -0.139982 -0.618438 -0.053906   \n4     -0.019113 -0.312889 -0.11205 -0.028606 -0.139982 -0.618438 -0.053906   \n...         ...       ...      ...       ...       ...       ...       ...   \n15080 -0.019113  3.196020 -0.11205 -0.028606 -0.139982 -0.618438 -0.053906   \n15081 -0.019113 -0.312889 -0.11205 -0.028606 -0.139982 -0.618438 -0.053906   \n15082 -0.019113 -0.312889 -0.11205 -0.028606 -0.139982  1.616978 -0.053906   \n15083 -0.019113 -0.312889 -0.11205 -0.028606 -0.139982 -0.618438 -0.053906   \n15084 -0.019113  3.196020 -0.11205 -0.028606 -0.139982 -0.618438 -0.053906   \n\n             7         8         9   ...        85        86        87  \\\n0     -0.031767 -0.019726  0.825150  ... -0.227350  1.493939 -0.111426   \n1     -0.031767 -0.019726  0.825150  ... -0.439078  2.756092  2.198385   \n2     -0.031767 -0.019726 -1.211901  ... -0.439078 -0.447834 -0.022587   \n3     -0.031767 -0.019726  0.825150  ...  3.372038 -0.480197 -0.289103   \n4     -0.031767 -0.019726  0.825150  ... -0.439078 -0.447834 -0.022587   \n...         ...       ...       ...  ...       ...       ...       ...   \n15080 -0.031767 -0.019726 -1.211901  ... -0.439078  2.756092 -0.289103   \n15081 -0.031767 -0.019726  0.825150  ... -0.121485 -0.447834 -0.200265   \n15082 -0.031767 -0.019726 -1.211901  ... -0.439078 -0.447834 -0.200265   \n15083 -0.031767 -0.019726  0.825150  ... -0.439078 -0.480197 -0.289103   \n15084 -0.031767 -0.019726 -1.211901  ... -0.386146 -0.480197 -0.289103   \n\n             88        89        90        91        92        93        94  \n0     -0.639532 -0.624871 -0.387635 -0.376387  4.392317  2.629798  0.399301  \n1     -0.639532 -0.624871 -0.387635 -0.376387  4.787500  5.696764  0.267174  \n2     -0.617049 -0.624871 -0.387635 -0.376387  2.859691  2.038686  0.756717  \n3     -0.639532 -0.624871  1.961037 -0.251175  6.459633  5.691038  0.304314  \n4     -0.639532 -0.624871 -0.387635 -0.376387  1.253655  1.928996  0.776313  \n...         ...       ...       ...       ...       ...       ...       ...  \n15080 -0.639532 -0.624871 -0.387635 -0.376387  0.623444  3.422799  0.530172  \n15081 -0.617049 -0.624871 -0.387635 -0.376387  3.465312  2.150762  0.629530  \n15082 -0.617049 -0.624871 -0.387635 -0.376387  1.961879  2.013232  0.761405  \n15083 -0.639532 -0.624871 -0.159292 -0.157266  1.731618  2.150925  0.753762  \n15084 -0.639532 -0.624871 -0.387635 -0.376387  0.708319  2.325465  0.669863  \n\n[15085 rows x 95 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>85</th>\n      <th>86</th>\n      <th>87</th>\n      <th>88</th>\n      <th>89</th>\n      <th>90</th>\n      <th>91</th>\n      <th>92</th>\n      <th>93</th>\n      <th>94</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>-0.618438</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>0.825150</td>\n      <td>...</td>\n      <td>-0.227350</td>\n      <td>1.493939</td>\n      <td>-0.111426</td>\n      <td>-0.639532</td>\n      <td>-0.624871</td>\n      <td>-0.387635</td>\n      <td>-0.376387</td>\n      <td>4.392317</td>\n      <td>2.629798</td>\n      <td>0.399301</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>-0.618438</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>0.825150</td>\n      <td>...</td>\n      <td>-0.439078</td>\n      <td>2.756092</td>\n      <td>2.198385</td>\n      <td>-0.639532</td>\n      <td>-0.624871</td>\n      <td>-0.387635</td>\n      <td>-0.376387</td>\n      <td>4.787500</td>\n      <td>5.696764</td>\n      <td>0.267174</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>1.616978</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>-1.211901</td>\n      <td>...</td>\n      <td>-0.439078</td>\n      <td>-0.447834</td>\n      <td>-0.022587</td>\n      <td>-0.617049</td>\n      <td>-0.624871</td>\n      <td>-0.387635</td>\n      <td>-0.376387</td>\n      <td>2.859691</td>\n      <td>2.038686</td>\n      <td>0.756717</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>-0.618438</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>0.825150</td>\n      <td>...</td>\n      <td>3.372038</td>\n      <td>-0.480197</td>\n      <td>-0.289103</td>\n      <td>-0.639532</td>\n      <td>-0.624871</td>\n      <td>1.961037</td>\n      <td>-0.251175</td>\n      <td>6.459633</td>\n      <td>5.691038</td>\n      <td>0.304314</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>-0.618438</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>0.825150</td>\n      <td>...</td>\n      <td>-0.439078</td>\n      <td>-0.447834</td>\n      <td>-0.022587</td>\n      <td>-0.639532</td>\n      <td>-0.624871</td>\n      <td>-0.387635</td>\n      <td>-0.376387</td>\n      <td>1.253655</td>\n      <td>1.928996</td>\n      <td>0.776313</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>15080</th>\n      <td>-0.019113</td>\n      <td>3.196020</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>-0.618438</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>-1.211901</td>\n      <td>...</td>\n      <td>-0.439078</td>\n      <td>2.756092</td>\n      <td>-0.289103</td>\n      <td>-0.639532</td>\n      <td>-0.624871</td>\n      <td>-0.387635</td>\n      <td>-0.376387</td>\n      <td>0.623444</td>\n      <td>3.422799</td>\n      <td>0.530172</td>\n    </tr>\n    <tr>\n      <th>15081</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>-0.618438</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>0.825150</td>\n      <td>...</td>\n      <td>-0.121485</td>\n      <td>-0.447834</td>\n      <td>-0.200265</td>\n      <td>-0.617049</td>\n      <td>-0.624871</td>\n      <td>-0.387635</td>\n      <td>-0.376387</td>\n      <td>3.465312</td>\n      <td>2.150762</td>\n      <td>0.629530</td>\n    </tr>\n    <tr>\n      <th>15082</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>1.616978</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>-1.211901</td>\n      <td>...</td>\n      <td>-0.439078</td>\n      <td>-0.447834</td>\n      <td>-0.200265</td>\n      <td>-0.617049</td>\n      <td>-0.624871</td>\n      <td>-0.387635</td>\n      <td>-0.376387</td>\n      <td>1.961879</td>\n      <td>2.013232</td>\n      <td>0.761405</td>\n    </tr>\n    <tr>\n      <th>15083</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>-0.618438</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>0.825150</td>\n      <td>...</td>\n      <td>-0.439078</td>\n      <td>-0.480197</td>\n      <td>-0.289103</td>\n      <td>-0.639532</td>\n      <td>-0.624871</td>\n      <td>-0.159292</td>\n      <td>-0.157266</td>\n      <td>1.731618</td>\n      <td>2.150925</td>\n      <td>0.753762</td>\n    </tr>\n    <tr>\n      <th>15084</th>\n      <td>-0.019113</td>\n      <td>3.196020</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>-0.618438</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>-1.211901</td>\n      <td>...</td>\n      <td>-0.386146</td>\n      <td>-0.480197</td>\n      <td>-0.289103</td>\n      <td>-0.639532</td>\n      <td>-0.624871</td>\n      <td>-0.387635</td>\n      <td>-0.376387</td>\n      <td>0.708319</td>\n      <td>2.325465</td>\n      <td>0.669863</td>\n    </tr>\n  </tbody>\n</table>\n<p>15085 rows × 95 columns</p>\n</div>"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anomalous_test_data_with_Z_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "def getFinalPredictions(first_predictions, second_predictions):\n",
    "    f_p_copy = first_predictions.copy()\n",
    "    s_p_copy = second_predictions.copy()\n",
    "    indices = f_p_copy.index[f_p_copy[\"predicted_as_anomaly\"] == True]\n",
    "    s_p_copy.index = indices\n",
    "    f_p_copy.loc[indices, \"predicted_as_anomaly\"] = s_p_copy[\"predicted_as_anomaly\"]\n",
    "    print_results(f_p_copy)\n",
    "    print(\"auroc: \" + str(get_auroc_value(f_p_copy)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "from turtleIsolationForests.extendedIsolationForest import ExtendedIsolationForest\n",
    "from turtleIsolationForests.printResults import print_results\n",
    "from turtleIsolationForests.printResults import get_auroc_value\n",
    "\n",
    "eif = ExtendedIsolationForest(contamination = contamination, random_state = None)\n",
    "eif.fit(train_data_with_Z_df, train_labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.7 s ± 547 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "eif.predict(anomalous_test_data_with_Z_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true anomalies: 5691\n",
      "false anomalies: 4411\n",
      "false normals: 4020\n",
      "true normals: 8422\n",
      "precision: 0.5633537913284498\n",
      "recall: 0.586036453506333\n",
      "f1-score: 0.5744713067178115\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'anomaly_score'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[1;32mC:\\CodingProjects\\network-anomaly-detection\\venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key, method, tolerance)\u001B[0m\n\u001B[0;32m   3801\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 3802\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3803\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[1;32mC:\\CodingProjects\\network-anomaly-detection\\venv\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mC:\\CodingProjects\\network-anomaly-detection\\venv\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'anomaly_score'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[36], line 5\u001B[0m\n\u001B[0;32m      2\u001B[0m eif_predictions[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mis_normal\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m predicted_anomalous_labels\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m#print_results(eif_predictions)\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m \u001B[43mgetFinalPredictions\u001B[49m\u001B[43m(\u001B[49m\u001B[43mautoec_predictions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meif_predictions\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[33], line 8\u001B[0m, in \u001B[0;36mgetFinalPredictions\u001B[1;34m(first_predictions, second_predictions)\u001B[0m\n\u001B[0;32m      6\u001B[0m f_p_copy\u001B[38;5;241m.\u001B[39mloc[indices, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpredicted_as_anomaly\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m s_p_copy[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpredicted_as_anomaly\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m      7\u001B[0m print_results(f_p_copy)\n\u001B[1;32m----> 8\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauroc: \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(\u001B[43mget_auroc_value\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf_p_copy\u001B[49m\u001B[43m)\u001B[49m))\n",
      "File \u001B[1;32mC:\\CodingProjects\\network-anomaly-detection\\turtleIsolationForests\\printResults.py:33\u001B[0m, in \u001B[0;36mget_auroc_value\u001B[1;34m(predictions)\u001B[0m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_auroc_value\u001B[39m(predictions: DataFrame) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mfloat\u001B[39m:\n\u001B[1;32m---> 33\u001B[0m     (fpr, tpr, _) \u001B[38;5;241m=\u001B[39m \u001B[43mget_auroc_points\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpredictions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     34\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m get_auc(fpr, tpr)\n",
      "File \u001B[1;32mC:\\CodingProjects\\network-anomaly-detection\\turtleIsolationForests\\printResults.py:38\u001B[0m, in \u001B[0;36mget_auroc_points\u001B[1;34m(predictions)\u001B[0m\n\u001B[0;32m     36\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_auroc_points\u001B[39m(predictions: DataFrame) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mtuple\u001B[39m[np\u001B[38;5;241m.\u001B[39mndarray, np\u001B[38;5;241m.\u001B[39mndarray, np\u001B[38;5;241m.\u001B[39mndarray]:\n\u001B[0;32m     37\u001B[0m     y_true \u001B[38;5;241m=\u001B[39m predictions[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mis_normal\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m---> 38\u001B[0m     y_score \u001B[38;5;241m=\u001B[39m \u001B[43mpredictions\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43manomaly_score\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[0;32m     39\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m roc_curve(y_true, y_score, pos_label\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[1;32mC:\\CodingProjects\\network-anomaly-detection\\venv\\lib\\site-packages\\pandas\\core\\frame.py:3807\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3805\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m   3806\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[1;32m-> 3807\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3808\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[0;32m   3809\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
      "File \u001B[1;32mC:\\CodingProjects\\network-anomaly-detection\\venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key, method, tolerance)\u001B[0m\n\u001B[0;32m   3802\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine\u001B[38;5;241m.\u001B[39mget_loc(casted_key)\n\u001B[0;32m   3803\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[1;32m-> 3804\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[0;32m   3805\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m   3806\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[0;32m   3807\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[0;32m   3808\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[0;32m   3809\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
      "\u001B[1;31mKeyError\u001B[0m: 'anomaly_score'"
     ]
    }
   ],
   "source": [
    "eif_predictions = eif.predict(anomalous_test_data_with_Z_df)\n",
    "eif_predictions['is_normal'] = predicted_anomalous_labels\n",
    "\n",
    "#print_results(eif_predictions)\n",
    "getFinalPredictions(autoec_predictions, eif_predictions)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_predictions = eif.train_scores\n",
    "train_predictions['is_normal'] = train_labels\n",
    "print_results(train_predictions)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from turtleIsolationForests.sciForest import SCIsolationForest\n",
    "\n",
    "scif = SCIsolationForest(contamination = contamination, num_hyperplanes_per_split=5, random_state = None)\n",
    "scif.fit(train_data_with_Z_df, train_labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "scif.predict(anomalous_test_data_with_Z_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scif_predictions = scif.predict(anomalous_test_data_with_Z_df)\n",
    "scif_predictions['is_normal'] = predicted_anomalous_labels\n",
    "\n",
    "#print_results(scif_predictions)\n",
    "getFinalPredictions(autoec_predictions, scif_predictions)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from turtleIsolationForests.isolationForest import IsolationForest\n",
    "\n",
    "isoforest = IsolationForest(contamination = contamination, random_state = None)\n",
    "isoforest.fit(train_data_with_Z_df, train_labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "isoforest.predict(anomalous_test_data_with_Z_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "isoforest_predictions = isoforest.predict(anomalous_test_data_with_Z_df)\n",
    "isoforest_predictions['is_normal'] = predicted_anomalous_labels\n",
    "\n",
    "#print_results(isoforest_predictions)\n",
    "getFinalPredictions(autoec_predictions, isoforest_predictions)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from turtleIsolationForests.FBIF import FBIsolationForest\n",
    "\n",
    "fbif = FBIsolationForest(contamination = contamination, random_state = None)\n",
    "fbif.fit(train_data_with_Z_df, train_labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "fbif.predict(anomalous_test_data_with_Z_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fbif_predictions = fbif.predict(anomalous_test_data_with_Z_df)\n",
    "fbif_predictions['is_normal'] = predicted_anomalous_labels\n",
    "\n",
    "#print_results(fbif_predictions)\n",
    "getFinalPredictions(autoec_predictions, fbif_predictions)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
