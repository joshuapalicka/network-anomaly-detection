{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 333,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "outputs": [],
   "source": [
    "import autoencoder.aecExtraFeatures as Z_calculations\n",
    "\n",
    "def addZToPrediction(model, data_point):\n",
    "    encoded = model.encoder(data_point)\n",
    "    reconstruction = model.decoder(encoded)\n",
    "\n",
    "    Z_features = [Z_calculations.getZVector(data_point, reconstruction, encoded)]\n",
    "\n",
    "    Z_features_tensor = tf.convert_to_tensor(Z_features, dtype=tf.float32)\n",
    "    data_point = tf.convert_to_tensor(data_point, dtype=tf.float32)\n",
    "\n",
    "    data_point = tf.concat([data_point, Z_features_tensor], 1)\n",
    "\n",
    "    return data_point"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "outputs": [],
   "source": [
    "def isAnomaly(data_point, model_1, model_2, threshold):\n",
    "\n",
    "    # need autoencoder to return boolean isAnomaly\n",
    "    isAnomaly = tf.math.less(tf.keras.losses.mae(model_1(data_point), data_point), threshold)\n",
    "\n",
    "    # if the autoencoder doesn't find anything out of the ordinary, return False\n",
    "    if not isAnomaly:\n",
    "        return False\n",
    "\n",
    "    data_point = addZToPrediction(model_1, data_point)\n",
    "\n",
    "    # if the autoencoder sees something weird, run it through the isolation forest to make sure\n",
    "    return model_2.predict(data_point)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "outputs": [],
   "source": [
    "from turtleIsolationForests.preprocessFeatures import preprocess_features\n",
    "\n",
    "train_dataframe = pd.read_csv(\"eda_simple_classification/network_data_mod_train.csv\", index_col=0)\n",
    "test_dataframe = pd.read_csv(\"eda_simple_classification/network_data_mod_test.csv\", index_col=0)\n",
    "\n",
    "train_data, test_data, train_labels, test_labels = preprocess_features(train_dataframe, test_dataframe)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "outputs": [],
   "source": [
    "train_data, test_data, train_labels, test_labels = train_data[:1000], test_data[:1000], train_labels[:1000], test_labels[:1000]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "outputs": [],
   "source": [
    "np_train_labels = train_labels.to_numpy()\n",
    "np_test_labels = test_labels.to_numpy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "outputs": [],
   "source": [
    "np_train_data = train_data.to_numpy()\n",
    "np_test_data = test_data.to_numpy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "outputs": [],
   "source": [
    "np_train_data = tf.cast(np_train_data, tf.float32)\n",
    "np_test_data = tf.cast(np_test_data, tf.float32)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "outputs": [],
   "source": [
    "np_train_labels = np_train_labels.astype(bool)\n",
    "np_test_labels = np_test_labels.astype(bool)\n",
    "\n",
    "normal_train_data = np_train_data[np_train_labels]\n",
    "normal_test_data = np_test_data[np_test_labels]\n",
    "\n",
    "anomalous_train_data = np_train_data[~np_train_labels]\n",
    "anomalous_test_data = np_test_data[~np_test_labels]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "outputs": [],
   "source": [
    "from autoencoder.autoencoder import AnomalyDetector\n",
    "autoencoder = AnomalyDetector()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adam', loss='mae')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Tensor(\"anomaly_detector_6/sequential_12/dense_38/Relu:0\", shape=(None, 8), dtype=float32)\n",
      "Tensor(\"anomaly_detector_6/sequential_13/dense_41/Sigmoid:0\", shape=(None, 46), dtype=float32)\n",
      "Tensor(\"anomaly_detector_6/sequential_12/dense_38/Relu:0\", shape=(None, 8), dtype=float32)\n",
      "Tensor(\"anomaly_detector_6/sequential_13/dense_41/Sigmoid:0\", shape=(None, 46), dtype=float32)\n",
      " 1/17 [>.............................] - ETA: 25s - loss: 0.7303Tensor(\"anomaly_detector_6/sequential_12/dense_38/Relu:0\", shape=(None, 8), dtype=float32)\n",
      "Tensor(\"anomaly_detector_6/sequential_13/dense_41/Sigmoid:0\", shape=(None, 46), dtype=float32)\n",
      "17/17 [==============================] - 2s 23ms/step - loss: 0.7158 - val_loss: 0.7816\n",
      "Epoch 2/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.6390 - val_loss: 0.6773\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.4685 - val_loss: 0.5486\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.3627 - val_loss: 0.5038\n",
      "Epoch 5/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.3258 - val_loss: 0.4744\n",
      "Epoch 6/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.3021 - val_loss: 0.4663\n",
      "Epoch 7/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2993 - val_loss: 0.4647\n",
      "Epoch 8/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.2988 - val_loss: 0.4634\n",
      "Epoch 9/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.2987 - val_loss: 0.4627\n",
      "Epoch 10/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2984 - val_loss: 0.4622\n",
      "Epoch 11/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2983 - val_loss: 0.4619\n",
      "Epoch 12/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2983 - val_loss: 0.4613\n",
      "Epoch 13/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2981 - val_loss: 0.4611\n",
      "Epoch 14/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2981 - val_loss: 0.4608\n",
      "Epoch 15/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2980 - val_loss: 0.4607\n",
      "Epoch 16/100\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.2979 - val_loss: 0.4600\n",
      "Epoch 17/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2978 - val_loss: 0.4595\n",
      "Epoch 18/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2977 - val_loss: 0.4591\n",
      "Epoch 19/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2975 - val_loss: 0.4584\n",
      "Epoch 20/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2973 - val_loss: 0.4581\n",
      "Epoch 21/100\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.2971 - val_loss: 0.4576\n",
      "Epoch 22/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2969 - val_loss: 0.4571\n",
      "Epoch 23/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2967 - val_loss: 0.4567\n",
      "Epoch 24/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2966 - val_loss: 0.4561\n",
      "Epoch 25/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.2964 - val_loss: 0.4555\n",
      "Epoch 26/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2961 - val_loss: 0.4528\n",
      "Epoch 27/100\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.2952 - val_loss: 0.4482\n",
      "Epoch 28/100\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.2942 - val_loss: 0.4470\n",
      "Epoch 29/100\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.2937 - val_loss: 0.4463\n",
      "Epoch 30/100\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.2927 - val_loss: 0.4450\n",
      "Epoch 31/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2914 - val_loss: 0.4433\n",
      "Epoch 32/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2898 - val_loss: 0.4403\n",
      "Epoch 33/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2872 - val_loss: 0.4374\n",
      "Epoch 34/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2860 - val_loss: 0.4364\n",
      "Epoch 35/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2851 - val_loss: 0.4359\n",
      "Epoch 36/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2848 - val_loss: 0.4358\n",
      "Epoch 37/100\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.2846 - val_loss: 0.4353\n",
      "Epoch 38/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2843 - val_loss: 0.4353\n",
      "Epoch 39/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2841 - val_loss: 0.4347\n",
      "Epoch 40/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2839 - val_loss: 0.4348\n",
      "Epoch 41/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2836 - val_loss: 0.4346\n",
      "Epoch 42/100\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.2834 - val_loss: 0.4341\n",
      "Epoch 43/100\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.2832 - val_loss: 0.4337\n",
      "Epoch 44/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2830 - val_loss: 0.4333\n",
      "Epoch 45/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2830 - val_loss: 0.4332\n",
      "Epoch 46/100\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.2829 - val_loss: 0.4331\n",
      "Epoch 47/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2830 - val_loss: 0.4332\n",
      "Epoch 48/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2829 - val_loss: 0.4323\n",
      "Epoch 49/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2827 - val_loss: 0.4327\n",
      "Epoch 50/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2827 - val_loss: 0.4321\n",
      "Epoch 51/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2826 - val_loss: 0.4320\n",
      "Epoch 52/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2826 - val_loss: 0.4319\n",
      "Epoch 53/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2824 - val_loss: 0.4319\n",
      "Epoch 54/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2824 - val_loss: 0.4316\n",
      "Epoch 55/100\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.2822 - val_loss: 0.4316\n",
      "Epoch 56/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2823 - val_loss: 0.4320\n",
      "Epoch 57/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2823 - val_loss: 0.4313\n",
      "Epoch 58/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2821 - val_loss: 0.4312\n",
      "Epoch 59/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2822 - val_loss: 0.4309\n",
      "Epoch 60/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2821 - val_loss: 0.4308\n",
      "Epoch 61/100\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.2820 - val_loss: 0.4312\n",
      "Epoch 62/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2820 - val_loss: 0.4308\n",
      "Epoch 63/100\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.2819 - val_loss: 0.4309\n",
      "Epoch 64/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2819 - val_loss: 0.4303\n",
      "Epoch 65/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2818 - val_loss: 0.4307\n",
      "Epoch 66/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2818 - val_loss: 0.4306\n",
      "Epoch 67/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2817 - val_loss: 0.4304\n",
      "Epoch 68/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2816 - val_loss: 0.4304\n",
      "Epoch 69/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2816 - val_loss: 0.4301\n",
      "Epoch 70/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2816 - val_loss: 0.4304\n",
      "Epoch 71/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2815 - val_loss: 0.4303\n",
      "Epoch 72/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2814 - val_loss: 0.4300\n",
      "Epoch 73/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.2813 - val_loss: 0.4299\n",
      "Epoch 74/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2813 - val_loss: 0.4302\n",
      "Epoch 75/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2814 - val_loss: 0.4298\n",
      "Epoch 76/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.2813 - val_loss: 0.4298\n",
      "Epoch 77/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2813 - val_loss: 0.4297\n",
      "Epoch 78/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2814 - val_loss: 0.4294\n",
      "Epoch 79/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2814 - val_loss: 0.4298\n",
      "Epoch 80/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2811 - val_loss: 0.4296\n",
      "Epoch 81/100\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.2811 - val_loss: 0.4299\n",
      "Epoch 82/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2810 - val_loss: 0.4297\n",
      "Epoch 83/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2812 - val_loss: 0.4299\n",
      "Epoch 84/100\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.2814 - val_loss: 0.4295\n",
      "Epoch 85/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.2812 - val_loss: 0.4297\n",
      "Epoch 86/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2811 - val_loss: 0.4295\n",
      "Epoch 87/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2809 - val_loss: 0.4296\n",
      "Epoch 88/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2810 - val_loss: 0.4297\n",
      "Epoch 89/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2811 - val_loss: 0.4294\n",
      "Epoch 90/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2809 - val_loss: 0.4294\n",
      "Epoch 91/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2808 - val_loss: 0.4292\n",
      "Epoch 92/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2807 - val_loss: 0.4291\n",
      "Epoch 93/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2807 - val_loss: 0.4291\n",
      "Epoch 94/100\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.2807 - val_loss: 0.4289\n",
      "Epoch 95/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2808 - val_loss: 0.4290\n",
      "Epoch 96/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2808 - val_loss: 0.4292\n",
      "Epoch 97/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2810 - val_loss: 0.4290\n",
      "Epoch 98/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2809 - val_loss: 0.4290\n",
      "Epoch 99/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2807 - val_loss: 0.4289\n",
      "Epoch 100/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2807 - val_loss: 0.4288\n"
     ]
    }
   ],
   "source": [
    "history = autoencoder.fit(normal_train_data, normal_train_data,\n",
    "          epochs=100,\n",
    "          validation_data=(test_data, test_data),\n",
    "          shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.title(\"Training Loss & Validation Loss over epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "#plt.ylim([0,.0008])\n",
    "plt.legend([\"Loss\", \"Validation Loss\"])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"anomaly_detector_6/sequential_12/dense_38/Relu:0\", shape=(None, 8), dtype=float32)\n",
      "Tensor(\"anomaly_detector_6/sequential_13/dense_41/Sigmoid:0\", shape=(None, 46), dtype=float32)\n",
      "17/17 [==============================] - 0s 1ms/step\n",
      "Threshold:  0.4064331\n"
     ]
    }
   ],
   "source": [
    "reconstructions = autoencoder.predict(normal_train_data)\n",
    "train_loss = tf.keras.losses.mae(reconstructions, normal_train_data)\n",
    "threshold = np.mean(train_loss) + np.std(train_loss)\n",
    "print(\"Threshold: \", threshold)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "outputs": [],
   "source": [
    "def predict(model, data, threshold):\n",
    "  reconstructions = model(data)\n",
    "  loss = tf.keras.losses.mae(data, reconstructions) # 0 = anomaly (same as data)\n",
    "  return tf.math.less(loss, threshold)\n",
    "\n",
    "def print_stats(predictions, labels):\n",
    "  print(\"Accuracy = {}\".format(accuracy_score(labels, predictions)))\n",
    "  print(\"Precision = {}\".format(precision_score(labels, predictions)))\n",
    "  print(\"Recall = {}\".format(recall_score(labels, predictions)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.6 ms ± 440 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "%%capture\n",
    "\n",
    "predict(autoencoder, np_test_data, threshold)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "test_predictions = predict(autoencoder, np_test_data, threshold)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Stats for whole dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.812\n",
      "Precision = 0.7391304347826086\n",
      "Recall = 0.9026548672566371\n"
     ]
    }
   ],
   "source": [
    "print_stats(test_predictions, test_labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[404 144]\n",
      " [ 44 408]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(test_labels, test_predictions))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "outputs": [],
   "source": [
    "test_labels_swap = []\n",
    "for i in range(len(test_labels)):\n",
    "    if test_labels[i]:\n",
    "        test_labels_swap.append(False)\n",
    "    else:\n",
    "        test_labels_swap.append(True)\n",
    "\n",
    "test_labels = test_labels_swap\n",
    "np_test_labels = test_labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Stats for all predicted anomalous data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true anomalies: 408\n",
      "false anomalies: 144\n",
      "false normals: 44\n",
      "true normals: 404\n",
      "precision: 0.7391304347826086\n",
      "recall: 0.9026548672566371\n",
      "f1-score: 0.8127490039840637\n"
     ]
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "from turtleIsolationForests.printResults import print_results\n",
    "\n",
    "test_predictions_np = test_predictions.numpy()\n",
    "\n",
    "autoec_predictions = DataFrame()\n",
    "\n",
    "autoec_predictions['predicted_as_anomaly'] = test_predictions_np\n",
    "autoec_predictions['is_normal'] = test_labels\n",
    "\n",
    "print_results(autoec_predictions)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "outputs": [],
   "source": [
    "def addZToData(data, model):\n",
    "    data_with_Z = []\n",
    "    for i in range(1, len(data)+1):\n",
    "        data_with_Z.append(addZToPrediction(model, data[i-1:i]))\n",
    "\n",
    "    data_with_Z_rf = []\n",
    "    for i in range(len(data_with_Z)):\n",
    "        data_with_Z_rf.append(np.append(np_train_data[:][:][i].numpy().reshape(1,46).squeeze(), data_with_Z[i]))\n",
    "\n",
    "    return pd.DataFrame(data_with_Z_rf)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "outputs": [],
   "source": [
    "train_data_with_Z_df = addZToData(np_train_data, autoencoder)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "outputs": [
    {
     "data": {
      "text/plain": "           0         1        2         3         4         5         6   \\\n0   -0.019113 -0.312889 -0.11205 -0.028606 -0.139982 -0.618438 -0.053906   \n1   -0.019113 -0.312889 -0.11205 -0.028606 -0.139982 -0.618438 -0.053906   \n2   -0.019113 -0.312889 -0.11205 -0.028606 -0.139982  1.616978 -0.053906   \n3   -0.019113 -0.312889 -0.11205 -0.028606 -0.139982 -0.618438 -0.053906   \n4   -0.019113 -0.312889 -0.11205 -0.028606 -0.139982 -0.618438 -0.053906   \n..        ...       ...      ...       ...       ...       ...       ...   \n995 -0.019113 -0.312889 -0.11205 -0.028606  7.143771 -0.618438 -0.053906   \n996 -0.019113 -0.312889 -0.11205 -0.028606 -0.139982 -0.618438 -0.053906   \n997 -0.019113 -0.312889 -0.11205 -0.028606 -0.139982  1.616978 -0.053906   \n998 -0.019113 -0.312889 -0.11205 -0.028606 -0.139982  1.616978 -0.053906   \n999 -0.019113 -0.312889 -0.11205 -0.028606 -0.139982 -0.618438 -0.053906   \n\n           7         8         9   ...        85        86        87  \\\n0   -0.031767 -0.019726  0.825150  ... -0.280282  0.069972 -0.289103   \n1   -0.031767 -0.019726  0.825150  ...  2.736852  2.367737 -0.289103   \n2   -0.031767 -0.019726 -1.211901  ... -0.174417 -0.480197 -0.289103   \n3   -0.031767 -0.019726  0.825150  ... -0.439078 -0.383108  0.066252   \n4   -0.031767 -0.019726  0.825150  ... -0.439078 -0.480197 -0.289103   \n..        ...       ...       ...  ...       ...       ...       ...   \n995 -0.031767 -0.019726 -1.211901  ... -0.121485 -0.447834 -0.289103   \n996 -0.031767 -0.019726  0.825150  ... -0.439078 -0.350745  0.066252   \n997 -0.031767 -0.019726 -1.211901  ... -0.068553 -0.480197 -0.289103   \n998 -0.031767 -0.019726 -1.211901  ... -0.068553 -0.480197 -0.289103   \n999 -0.031767 -0.019726  0.825150  ... -0.439078  0.167061  0.155091   \n\n           88        89        90        91        92         93        94  \n0   -0.639532 -0.624871 -0.224532 -0.376387  0.000000   2.897089  0.116624  \n1   -0.639532 -0.624871 -0.387635 -0.376387  0.000000   4.931060  0.052502  \n2    1.608759  1.618955 -0.387635 -0.376387  0.000000   5.080774 -0.196176  \n3   -0.572083 -0.602433 -0.387635 -0.345084  3.706993   2.350453  0.708277  \n4   -0.639532 -0.624871 -0.387635 -0.376387  4.880975   2.062329  0.752867  \n..        ...       ...       ...       ...       ...        ...       ...  \n995  0.866823 -0.490241 -0.355014 -0.000751  0.000000  13.498476  0.049060  \n996 -0.639532 -0.624871 -0.387635 -0.376387  3.674186   2.560726  0.676336  \n997  1.608759  1.618955 -0.387635 -0.376387  0.000000   5.133149 -0.200215  \n998  1.608759  1.618955 -0.387635 -0.376387  0.091040   5.355477 -0.173446  \n999 -0.639532 -0.602433 -0.387635 -0.376387  3.406525   2.672737  0.660340  \n\n[1000 rows x 95 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>85</th>\n      <th>86</th>\n      <th>87</th>\n      <th>88</th>\n      <th>89</th>\n      <th>90</th>\n      <th>91</th>\n      <th>92</th>\n      <th>93</th>\n      <th>94</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>-0.618438</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>0.825150</td>\n      <td>...</td>\n      <td>-0.280282</td>\n      <td>0.069972</td>\n      <td>-0.289103</td>\n      <td>-0.639532</td>\n      <td>-0.624871</td>\n      <td>-0.224532</td>\n      <td>-0.376387</td>\n      <td>0.000000</td>\n      <td>2.897089</td>\n      <td>0.116624</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>-0.618438</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>0.825150</td>\n      <td>...</td>\n      <td>2.736852</td>\n      <td>2.367737</td>\n      <td>-0.289103</td>\n      <td>-0.639532</td>\n      <td>-0.624871</td>\n      <td>-0.387635</td>\n      <td>-0.376387</td>\n      <td>0.000000</td>\n      <td>4.931060</td>\n      <td>0.052502</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>1.616978</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>-1.211901</td>\n      <td>...</td>\n      <td>-0.174417</td>\n      <td>-0.480197</td>\n      <td>-0.289103</td>\n      <td>1.608759</td>\n      <td>1.618955</td>\n      <td>-0.387635</td>\n      <td>-0.376387</td>\n      <td>0.000000</td>\n      <td>5.080774</td>\n      <td>-0.196176</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>-0.618438</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>0.825150</td>\n      <td>...</td>\n      <td>-0.439078</td>\n      <td>-0.383108</td>\n      <td>0.066252</td>\n      <td>-0.572083</td>\n      <td>-0.602433</td>\n      <td>-0.387635</td>\n      <td>-0.345084</td>\n      <td>3.706993</td>\n      <td>2.350453</td>\n      <td>0.708277</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>-0.618438</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>0.825150</td>\n      <td>...</td>\n      <td>-0.439078</td>\n      <td>-0.480197</td>\n      <td>-0.289103</td>\n      <td>-0.639532</td>\n      <td>-0.624871</td>\n      <td>-0.387635</td>\n      <td>-0.376387</td>\n      <td>4.880975</td>\n      <td>2.062329</td>\n      <td>0.752867</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>995</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>7.143771</td>\n      <td>-0.618438</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>-1.211901</td>\n      <td>...</td>\n      <td>-0.121485</td>\n      <td>-0.447834</td>\n      <td>-0.289103</td>\n      <td>0.866823</td>\n      <td>-0.490241</td>\n      <td>-0.355014</td>\n      <td>-0.000751</td>\n      <td>0.000000</td>\n      <td>13.498476</td>\n      <td>0.049060</td>\n    </tr>\n    <tr>\n      <th>996</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>-0.618438</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>0.825150</td>\n      <td>...</td>\n      <td>-0.439078</td>\n      <td>-0.350745</td>\n      <td>0.066252</td>\n      <td>-0.639532</td>\n      <td>-0.624871</td>\n      <td>-0.387635</td>\n      <td>-0.376387</td>\n      <td>3.674186</td>\n      <td>2.560726</td>\n      <td>0.676336</td>\n    </tr>\n    <tr>\n      <th>997</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>1.616978</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>-1.211901</td>\n      <td>...</td>\n      <td>-0.068553</td>\n      <td>-0.480197</td>\n      <td>-0.289103</td>\n      <td>1.608759</td>\n      <td>1.618955</td>\n      <td>-0.387635</td>\n      <td>-0.376387</td>\n      <td>0.000000</td>\n      <td>5.133149</td>\n      <td>-0.200215</td>\n    </tr>\n    <tr>\n      <th>998</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>1.616978</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>-1.211901</td>\n      <td>...</td>\n      <td>-0.068553</td>\n      <td>-0.480197</td>\n      <td>-0.289103</td>\n      <td>1.608759</td>\n      <td>1.618955</td>\n      <td>-0.387635</td>\n      <td>-0.376387</td>\n      <td>0.091040</td>\n      <td>5.355477</td>\n      <td>-0.173446</td>\n    </tr>\n    <tr>\n      <th>999</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>-0.618438</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>0.825150</td>\n      <td>...</td>\n      <td>-0.439078</td>\n      <td>0.167061</td>\n      <td>0.155091</td>\n      <td>-0.639532</td>\n      <td>-0.602433</td>\n      <td>-0.387635</td>\n      <td>-0.376387</td>\n      <td>3.406525</td>\n      <td>2.672737</td>\n      <td>0.660340</td>\n    </tr>\n  </tbody>\n</table>\n<p>1000 rows × 95 columns</p>\n</div>"
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_with_Z_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "outputs": [],
   "source": [
    "np_test_labels = np.array(np_test_labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "outputs": [],
   "source": [
    "predicted_anomalous = np_test_data[test_predictions.numpy()]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "outputs": [
    {
     "data": {
      "text/plain": "           0         1        2         3         4         5         6   \\\n0   -0.019113 -0.312889 -0.11205 -0.028606 -0.139982 -0.618438 -0.053906   \n1   -0.019113 -0.312889 -0.11205 -0.028606 -0.139982 -0.618438 -0.053906   \n2   -0.019113 -0.312889 -0.11205 -0.028606 -0.139982  1.616978 -0.053906   \n3   -0.019113 -0.312889 -0.11205 -0.028606 -0.139982 -0.618438 -0.053906   \n4   -0.019113 -0.312889 -0.11205 -0.028606 -0.139982 -0.618438 -0.053906   \n..        ...       ...      ...       ...       ...       ...       ...   \n995 -0.019113 -0.312889 -0.11205 -0.028606  7.143771 -0.618438 -0.053906   \n996 -0.019113 -0.312889 -0.11205 -0.028606 -0.139982 -0.618438 -0.053906   \n997 -0.019113 -0.312889 -0.11205 -0.028606 -0.139982  1.616978 -0.053906   \n998 -0.019113 -0.312889 -0.11205 -0.028606 -0.139982  1.616978 -0.053906   \n999 -0.019113 -0.312889 -0.11205 -0.028606 -0.139982 -0.618438 -0.053906   \n\n           7         8         9   ...        85        86        87  \\\n0   -0.031767 -0.019726  0.825150  ... -0.280282  0.069972 -0.289103   \n1   -0.031767 -0.019726  0.825150  ...  2.736852  2.367737 -0.289103   \n2   -0.031767 -0.019726 -1.211901  ... -0.174417 -0.480197 -0.289103   \n3   -0.031767 -0.019726  0.825150  ... -0.439078 -0.383108  0.066252   \n4   -0.031767 -0.019726  0.825150  ... -0.439078 -0.480197 -0.289103   \n..        ...       ...       ...  ...       ...       ...       ...   \n995 -0.031767 -0.019726 -1.211901  ... -0.121485 -0.447834 -0.289103   \n996 -0.031767 -0.019726  0.825150  ... -0.439078 -0.350745  0.066252   \n997 -0.031767 -0.019726 -1.211901  ... -0.068553 -0.480197 -0.289103   \n998 -0.031767 -0.019726 -1.211901  ... -0.068553 -0.480197 -0.289103   \n999 -0.031767 -0.019726  0.825150  ... -0.439078  0.167061  0.155091   \n\n           88        89        90        91        92         93        94  \n0   -0.639532 -0.624871 -0.224532 -0.376387  0.000000   2.897089  0.116624  \n1   -0.639532 -0.624871 -0.387635 -0.376387  0.000000   4.931060  0.052502  \n2    1.608759  1.618955 -0.387635 -0.376387  0.000000   5.080774 -0.196176  \n3   -0.572083 -0.602433 -0.387635 -0.345084  3.706993   2.350453  0.708277  \n4   -0.639532 -0.624871 -0.387635 -0.376387  4.880975   2.062329  0.752867  \n..        ...       ...       ...       ...       ...        ...       ...  \n995  0.866823 -0.490241 -0.355014 -0.000751  0.000000  13.498476  0.049060  \n996 -0.639532 -0.624871 -0.387635 -0.376387  3.674186   2.560726  0.676336  \n997  1.608759  1.618955 -0.387635 -0.376387  0.000000   5.133149 -0.200215  \n998  1.608759  1.618955 -0.387635 -0.376387  0.091040   5.355477 -0.173446  \n999 -0.639532 -0.602433 -0.387635 -0.376387  3.406525   2.672737  0.660340  \n\n[1000 rows x 95 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>85</th>\n      <th>86</th>\n      <th>87</th>\n      <th>88</th>\n      <th>89</th>\n      <th>90</th>\n      <th>91</th>\n      <th>92</th>\n      <th>93</th>\n      <th>94</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>-0.618438</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>0.825150</td>\n      <td>...</td>\n      <td>-0.280282</td>\n      <td>0.069972</td>\n      <td>-0.289103</td>\n      <td>-0.639532</td>\n      <td>-0.624871</td>\n      <td>-0.224532</td>\n      <td>-0.376387</td>\n      <td>0.000000</td>\n      <td>2.897089</td>\n      <td>0.116624</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>-0.618438</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>0.825150</td>\n      <td>...</td>\n      <td>2.736852</td>\n      <td>2.367737</td>\n      <td>-0.289103</td>\n      <td>-0.639532</td>\n      <td>-0.624871</td>\n      <td>-0.387635</td>\n      <td>-0.376387</td>\n      <td>0.000000</td>\n      <td>4.931060</td>\n      <td>0.052502</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>1.616978</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>-1.211901</td>\n      <td>...</td>\n      <td>-0.174417</td>\n      <td>-0.480197</td>\n      <td>-0.289103</td>\n      <td>1.608759</td>\n      <td>1.618955</td>\n      <td>-0.387635</td>\n      <td>-0.376387</td>\n      <td>0.000000</td>\n      <td>5.080774</td>\n      <td>-0.196176</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>-0.618438</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>0.825150</td>\n      <td>...</td>\n      <td>-0.439078</td>\n      <td>-0.383108</td>\n      <td>0.066252</td>\n      <td>-0.572083</td>\n      <td>-0.602433</td>\n      <td>-0.387635</td>\n      <td>-0.345084</td>\n      <td>3.706993</td>\n      <td>2.350453</td>\n      <td>0.708277</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>-0.618438</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>0.825150</td>\n      <td>...</td>\n      <td>-0.439078</td>\n      <td>-0.480197</td>\n      <td>-0.289103</td>\n      <td>-0.639532</td>\n      <td>-0.624871</td>\n      <td>-0.387635</td>\n      <td>-0.376387</td>\n      <td>4.880975</td>\n      <td>2.062329</td>\n      <td>0.752867</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>995</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>7.143771</td>\n      <td>-0.618438</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>-1.211901</td>\n      <td>...</td>\n      <td>-0.121485</td>\n      <td>-0.447834</td>\n      <td>-0.289103</td>\n      <td>0.866823</td>\n      <td>-0.490241</td>\n      <td>-0.355014</td>\n      <td>-0.000751</td>\n      <td>0.000000</td>\n      <td>13.498476</td>\n      <td>0.049060</td>\n    </tr>\n    <tr>\n      <th>996</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>-0.618438</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>0.825150</td>\n      <td>...</td>\n      <td>-0.439078</td>\n      <td>-0.350745</td>\n      <td>0.066252</td>\n      <td>-0.639532</td>\n      <td>-0.624871</td>\n      <td>-0.387635</td>\n      <td>-0.376387</td>\n      <td>3.674186</td>\n      <td>2.560726</td>\n      <td>0.676336</td>\n    </tr>\n    <tr>\n      <th>997</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>1.616978</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>-1.211901</td>\n      <td>...</td>\n      <td>-0.068553</td>\n      <td>-0.480197</td>\n      <td>-0.289103</td>\n      <td>1.608759</td>\n      <td>1.618955</td>\n      <td>-0.387635</td>\n      <td>-0.376387</td>\n      <td>0.000000</td>\n      <td>5.133149</td>\n      <td>-0.200215</td>\n    </tr>\n    <tr>\n      <th>998</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>1.616978</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>-1.211901</td>\n      <td>...</td>\n      <td>-0.068553</td>\n      <td>-0.480197</td>\n      <td>-0.289103</td>\n      <td>1.608759</td>\n      <td>1.618955</td>\n      <td>-0.387635</td>\n      <td>-0.376387</td>\n      <td>0.091040</td>\n      <td>5.355477</td>\n      <td>-0.173446</td>\n    </tr>\n    <tr>\n      <th>999</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>-0.618438</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>0.825150</td>\n      <td>...</td>\n      <td>-0.439078</td>\n      <td>0.167061</td>\n      <td>0.155091</td>\n      <td>-0.639532</td>\n      <td>-0.602433</td>\n      <td>-0.387635</td>\n      <td>-0.376387</td>\n      <td>3.406525</td>\n      <td>2.672737</td>\n      <td>0.660340</td>\n    </tr>\n  </tbody>\n</table>\n<p>1000 rows × 95 columns</p>\n</div>"
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_with_Z_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "outputs": [],
   "source": [
    "predicted_anomalous_labels = np_test_labels[test_predictions.numpy()]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "outputs": [],
   "source": [
    "anomalous_test_data_with_Z_df = addZToData(predicted_anomalous, autoencoder)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "outputs": [],
   "source": [
    "contamination = sum(train_labels == 0) / len(train_labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "outputs": [],
   "source": [
    "# need test_predictions replaced at indices that autoencoder predicted anomalies"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "outputs": [
    {
     "data": {
      "text/plain": "           0         1        2         3         4         5         6   \\\n0   -0.019113 -0.312889 -0.11205 -0.028606 -0.139982 -0.618438 -0.053906   \n1   -0.019113 -0.312889 -0.11205 -0.028606 -0.139982 -0.618438 -0.053906   \n2   -0.019113 -0.312889 -0.11205 -0.028606 -0.139982  1.616978 -0.053906   \n3   -0.019113 -0.312889 -0.11205 -0.028606 -0.139982 -0.618438 -0.053906   \n4   -0.019113 -0.312889 -0.11205 -0.028606 -0.139982 -0.618438 -0.053906   \n..        ...       ...      ...       ...       ...       ...       ...   \n547 -0.019113 -0.312889 -0.11205 -0.028606 -0.139982  1.616978 -0.053906   \n548 -0.019113 -0.312889 -0.11205 -0.028606 -0.139982 -0.618438 -0.053906   \n549 -0.019113 -0.312889 -0.11205 -0.028606 -0.139982  1.616978 -0.053906   \n550 -0.019113 -0.312889 -0.11205 -0.028606 -0.139982 -0.618438 -0.053906   \n551 -0.019113 -0.312889 -0.11205 -0.028606 -0.139982 -0.618438 -0.053906   \n\n           7         8         9   ...        85        86        87  \\\n0   -0.031767 -0.019726  0.825150  ... -0.227350  1.493939 -0.111426   \n1   -0.031767 -0.019726  0.825150  ... -0.439078 -0.447834 -0.022587   \n2   -0.031767 -0.019726 -1.211901  ...  3.372038 -0.480197 -0.289103   \n3   -0.031767 -0.019726  0.825150  ... -0.439078 -0.447834 -0.022587   \n4   -0.031767 -0.019726  0.825150  ... -0.386146 -0.480197 -0.289103   \n..        ...       ...       ...  ...       ...       ...       ...   \n547 -0.031767 -0.019726 -1.211901  ... -0.439078 -0.480197 -0.289103   \n548 -0.031767 -0.019726  0.825150  ... -0.439078 -0.480197 -0.289103   \n549 -0.031767 -0.019726 -1.211901  ...  3.636698 -0.480197 -0.289103   \n550 -0.031767 -0.019726  0.825150  ... -0.386146 -0.480197 -0.289103   \n551 -0.031767 -0.019726  0.825150  ... -0.333214 -0.480197 -0.289103   \n\n           88        89        90        91        92        93        94  \n0   -0.639532 -0.624871 -0.387635 -0.376387  0.000000  3.088438  0.113477  \n1   -0.617049 -0.624871 -0.387635 -0.376387  4.124097  2.038764  0.756528  \n2   -0.639532 -0.624871  1.961037 -0.251175  0.000000  5.691982  0.304321  \n3   -0.639532 -0.624871 -0.387635 -0.376387  4.484724  1.946810  0.771582  \n4   -0.639532 -0.624871  1.765315  0.625309  0.000000  3.327319  0.102927  \n..        ...       ...       ...       ...       ...       ...       ...  \n547 -0.639532 -0.624871 -0.387635 -0.376387  4.475514  2.494858  0.685989  \n548 -0.639532 -0.624871 -0.387635 -0.376387  4.544272  2.197386  0.731250  \n549 -0.639532 -0.624871  2.026278 -0.376387  0.000000  4.806163  0.353027  \n550 -0.639532 -0.624871 -0.224532 -0.219872  4.171893  2.417590  0.697079  \n551  0.552062  0.519480 -0.355014 -0.345084  0.025543  2.796142  0.108683  \n\n[552 rows x 95 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>85</th>\n      <th>86</th>\n      <th>87</th>\n      <th>88</th>\n      <th>89</th>\n      <th>90</th>\n      <th>91</th>\n      <th>92</th>\n      <th>93</th>\n      <th>94</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>-0.618438</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>0.825150</td>\n      <td>...</td>\n      <td>-0.227350</td>\n      <td>1.493939</td>\n      <td>-0.111426</td>\n      <td>-0.639532</td>\n      <td>-0.624871</td>\n      <td>-0.387635</td>\n      <td>-0.376387</td>\n      <td>0.000000</td>\n      <td>3.088438</td>\n      <td>0.113477</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>-0.618438</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>0.825150</td>\n      <td>...</td>\n      <td>-0.439078</td>\n      <td>-0.447834</td>\n      <td>-0.022587</td>\n      <td>-0.617049</td>\n      <td>-0.624871</td>\n      <td>-0.387635</td>\n      <td>-0.376387</td>\n      <td>4.124097</td>\n      <td>2.038764</td>\n      <td>0.756528</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>1.616978</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>-1.211901</td>\n      <td>...</td>\n      <td>3.372038</td>\n      <td>-0.480197</td>\n      <td>-0.289103</td>\n      <td>-0.639532</td>\n      <td>-0.624871</td>\n      <td>1.961037</td>\n      <td>-0.251175</td>\n      <td>0.000000</td>\n      <td>5.691982</td>\n      <td>0.304321</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>-0.618438</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>0.825150</td>\n      <td>...</td>\n      <td>-0.439078</td>\n      <td>-0.447834</td>\n      <td>-0.022587</td>\n      <td>-0.639532</td>\n      <td>-0.624871</td>\n      <td>-0.387635</td>\n      <td>-0.376387</td>\n      <td>4.484724</td>\n      <td>1.946810</td>\n      <td>0.771582</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>-0.618438</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>0.825150</td>\n      <td>...</td>\n      <td>-0.386146</td>\n      <td>-0.480197</td>\n      <td>-0.289103</td>\n      <td>-0.639532</td>\n      <td>-0.624871</td>\n      <td>1.765315</td>\n      <td>0.625309</td>\n      <td>0.000000</td>\n      <td>3.327319</td>\n      <td>0.102927</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>547</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>1.616978</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>-1.211901</td>\n      <td>...</td>\n      <td>-0.439078</td>\n      <td>-0.480197</td>\n      <td>-0.289103</td>\n      <td>-0.639532</td>\n      <td>-0.624871</td>\n      <td>-0.387635</td>\n      <td>-0.376387</td>\n      <td>4.475514</td>\n      <td>2.494858</td>\n      <td>0.685989</td>\n    </tr>\n    <tr>\n      <th>548</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>-0.618438</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>0.825150</td>\n      <td>...</td>\n      <td>-0.439078</td>\n      <td>-0.480197</td>\n      <td>-0.289103</td>\n      <td>-0.639532</td>\n      <td>-0.624871</td>\n      <td>-0.387635</td>\n      <td>-0.376387</td>\n      <td>4.544272</td>\n      <td>2.197386</td>\n      <td>0.731250</td>\n    </tr>\n    <tr>\n      <th>549</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>1.616978</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>-1.211901</td>\n      <td>...</td>\n      <td>3.636698</td>\n      <td>-0.480197</td>\n      <td>-0.289103</td>\n      <td>-0.639532</td>\n      <td>-0.624871</td>\n      <td>2.026278</td>\n      <td>-0.376387</td>\n      <td>0.000000</td>\n      <td>4.806163</td>\n      <td>0.353027</td>\n    </tr>\n    <tr>\n      <th>550</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>-0.618438</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>0.825150</td>\n      <td>...</td>\n      <td>-0.386146</td>\n      <td>-0.480197</td>\n      <td>-0.289103</td>\n      <td>-0.639532</td>\n      <td>-0.624871</td>\n      <td>-0.224532</td>\n      <td>-0.219872</td>\n      <td>4.171893</td>\n      <td>2.417590</td>\n      <td>0.697079</td>\n    </tr>\n    <tr>\n      <th>551</th>\n      <td>-0.019113</td>\n      <td>-0.312889</td>\n      <td>-0.11205</td>\n      <td>-0.028606</td>\n      <td>-0.139982</td>\n      <td>-0.618438</td>\n      <td>-0.053906</td>\n      <td>-0.031767</td>\n      <td>-0.019726</td>\n      <td>0.825150</td>\n      <td>...</td>\n      <td>-0.333214</td>\n      <td>-0.480197</td>\n      <td>-0.289103</td>\n      <td>0.552062</td>\n      <td>0.519480</td>\n      <td>-0.355014</td>\n      <td>-0.345084</td>\n      <td>0.025543</td>\n      <td>2.796142</td>\n      <td>0.108683</td>\n    </tr>\n  </tbody>\n</table>\n<p>552 rows × 95 columns</p>\n</div>"
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anomalous_test_data_with_Z_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "outputs": [],
   "source": [
    "def getFinalPredictions(first_predictions, second_predictions):\n",
    "    f_p_copy = first_predictions\n",
    "    s_p_copy = second_predictions\n",
    "    indices = s_p_copy.index[f_p_copy[\"predicted_as_anomaly\"] == True]\n",
    "    s_p_copy.index = indices\n",
    "    f_p_copy.loc[indices, \"predicted_as_anomaly\"] = s_p_copy[\"predicted_as_anomaly\"]\n",
    "    print_results(f_p_copy)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished building forest\n",
      "Finished calculating threshold\n"
     ]
    }
   ],
   "source": [
    "from turtleIsolationForests.extendedIsolationForest import ExtendedIsolationForest\n",
    "from turtleIsolationForests.printResults import print_results\n",
    "\n",
    "eif = ExtendedIsolationForest(contamination = contamination, random_state = None)\n",
    "eif.fit(train_data_with_Z_df, train_labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.55 s ± 231 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "eif.predict(anomalous_test_data_with_Z_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along dimension 0; dimension is 552 but corresponding boolean dimension is 1000",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[368], line 5\u001B[0m\n\u001B[1;32m      2\u001B[0m eif_predictions[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mis_normal\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m predicted_anomalous_labels\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m#print_results(eif_predictions)\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m \u001B[43mgetFinalPredictions\u001B[49m\u001B[43m(\u001B[49m\u001B[43mautoec_predictions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meif_predictions\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[365], line 4\u001B[0m, in \u001B[0;36mgetFinalPredictions\u001B[0;34m(first_predictions, second_predictions)\u001B[0m\n\u001B[1;32m      2\u001B[0m f_p_copy \u001B[38;5;241m=\u001B[39m first_predictions\n\u001B[1;32m      3\u001B[0m s_p_copy \u001B[38;5;241m=\u001B[39m second_predictions\n\u001B[0;32m----> 4\u001B[0m indices \u001B[38;5;241m=\u001B[39m \u001B[43ms_p_copy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[43m[\u001B[49m\u001B[43mf_p_copy\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpredicted_as_anomaly\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m]\u001B[49m\n\u001B[1;32m      5\u001B[0m s_p_copy\u001B[38;5;241m.\u001B[39mindex \u001B[38;5;241m=\u001B[39m indices\n\u001B[1;32m      6\u001B[0m f_p_copy\u001B[38;5;241m.\u001B[39mloc[indices, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpredicted_as_anomaly\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m s_p_copy[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpredicted_as_anomaly\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m~/Documents/GitHub/network-anomaly-detection/venv/lib/python3.8/site-packages/pandas/core/indexes/range.py:979\u001B[0m, in \u001B[0;36mRangeIndex.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m    972\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIndexError\u001B[39;00m(\n\u001B[1;32m    973\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124monly integers, slices (`:`), \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    974\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mellipsis (`...`), numpy.newaxis (`None`) \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    975\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mand integer or boolean \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    976\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marrays are valid indices\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    977\u001B[0m     )\n\u001B[1;32m    978\u001B[0m \u001B[38;5;66;03m# fall back to Int64Index\u001B[39;00m\n\u001B[0;32m--> 979\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__getitem__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/GitHub/network-anomaly-detection/venv/lib/python3.8/site-packages/pandas/core/indexes/base.py:5339\u001B[0m, in \u001B[0;36mIndex.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   5336\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   5337\u001B[0m         key \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39masarray(key, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mbool\u001B[39m)\n\u001B[0;32m-> 5339\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mgetitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   5340\u001B[0m \u001B[38;5;66;03m# Because we ruled out integer above, we always get an arraylike here\u001B[39;00m\n\u001B[1;32m   5341\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m result\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n",
      "\u001B[0;31mIndexError\u001B[0m: boolean index did not match indexed array along dimension 0; dimension is 552 but corresponding boolean dimension is 1000"
     ]
    }
   ],
   "source": [
    "eif_predictions = eif.predict(anomalous_test_data_with_Z_df)\n",
    "eif_predictions['is_normal'] = predicted_anomalous_labels\n",
    "\n",
    "#print_results(eif_predictions)\n",
    "getFinalPredictions(autoec_predictions, eif_predictions)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished building forest\n",
      "Finished calculating threshold\n"
     ]
    }
   ],
   "source": [
    "from turtleIsolationForests.sciForest import SCIsolationForest\n",
    "\n",
    "scif = SCIsolationForest(contamination = contamination, num_hyperplanes_per_split=5, random_state = None)\n",
    "scif.fit(train_data_with_Z_df, train_labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1min 39s ± 704 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "scif.predict(anomalous_test_data_with_Z_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true anomalies: 4721\n",
      "false anomalies: 2480\n",
      "false normals: 4858\n",
      "true normals: 2465\n",
      "precision: 0.6556033884182753\n",
      "recall: 0.4928489403904374\n",
      "f1-score: 0.5626936829558998\n"
     ]
    }
   ],
   "source": [
    "scif_predictions = scif.predict(anomalous_test_data_with_Z_df)\n",
    "scif_predictions['is_normal'] = predicted_anomalous_labels\n",
    "\n",
    "#print_results(scif_predictions)\n",
    "getFinalPredictions(autoec_predictions, scif_predictions)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished building forest\n",
      "Finished calculating threshold\n"
     ]
    }
   ],
   "source": [
    "from turtleIsolationForests.isolationForest import IsolationForest\n",
    "\n",
    "isoforest = IsolationForest(contamination = contamination, random_state = None)\n",
    "isoforest.fit(train_data_with_Z_df, train_labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 s ± 590 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "isoforest.predict(anomalous_test_data_with_Z_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true anomalies: 4730\n",
      "false anomalies: 2503\n",
      "false normals: 4849\n",
      "true normals: 2442\n",
      "precision: 0.6539471865062906\n",
      "recall: 0.49378849566760624\n",
      "f1-score: 0.5626933142993099\n"
     ]
    }
   ],
   "source": [
    "isoforest_predictions = isoforest.predict(anomalous_test_data_with_Z_df)\n",
    "isoforest_predictions['is_normal'] = predicted_anomalous_labels\n",
    "\n",
    "#print_results(isoforest_predictions)\n",
    "getFinalPredictions(autoec_predictions, isoforest_predictions)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished building forest\n",
      "Finished calculating threshold\n"
     ]
    }
   ],
   "source": [
    "from turtleIsolationForests.FBIF import FBIsolationForest\n",
    "\n",
    "fbif = FBIsolationForest(contamination = contamination, random_state = None)\n",
    "fbif.fit(train_data_with_Z_df, train_labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2min 42s ± 1.53 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "fbif.predict(anomalous_test_data_with_Z_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true anomalies: 4758\n",
      "false anomalies: 2499\n",
      "false normals: 4821\n",
      "true normals: 2446\n",
      "precision: 0.6556428276147168\n",
      "recall: 0.49671155652990917\n",
      "f1-score: 0.5652173913043478\n"
     ]
    }
   ],
   "source": [
    "fbif_predictions = fbif.predict(anomalous_test_data_with_Z_df)\n",
    "fbif_predictions['is_normal'] = predicted_anomalous_labels\n",
    "\n",
    "#print_results(fbif_predictions)\n",
    "getFinalPredictions(autoec_predictions, fbif_predictions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}